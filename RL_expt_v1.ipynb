{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e36446b",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to run an experimental setup with Reinforcement Learning in the Ray library.\n",
    "\n",
    "An idea is to train an RL agent to play a simple game like Pokemon or Yugioh.\n",
    "\n",
    "https://docs.ray.io/en/latest/rllib/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7da588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e76c231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\rohit\\anaconda3\\lib\\site-packages\\ray\\rllib\\utils\\framework.py:126: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 17:35:40,653\tINFO util.py:154 -- Outdated packages:\n",
      "  ipywidgets==7.6.5 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "C:\\Users\\rohit\\anaconda3\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:525: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "C:\\Users\\rohit\\anaconda3\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "C:\\Users\\rohit\\anaconda3\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "C:\\Users\\rohit\\anaconda3\\lib\\site-packages\\ray\\tune\\logger\\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2024-05-30 17:35:41,961\tWARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!\n",
      "2024-05-30 17:35:45,800\tINFO worker.py:1749 -- Started a local Ray instance.\n",
      "\u001b[36m(pid=18764)\u001b[0m WARNING:tensorflow:From C:\\Users\\rohit\\anaconda3\\lib\\site-packages\\ray\\rllib\\utils\\framework.py:126: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\u001b[36m(pid=18764)\u001b[0m \n",
      "\u001b[36m(RolloutWorker pid=18764)\u001b[0m 2024-05-30 17:35:58,476\tWARNING deprecation.py:50 -- DeprecationWarning: `num_envs_per_worker` has been deprecated. Use `AlgorithmConfig.num_envs_per_env_runner` instead. This will raise an error in the future!\n",
      "2024-05-30 17:35:58,623\tWARNING deprecation.py:50 -- DeprecationWarning: `num_envs_per_worker` has been deprecated. Use `AlgorithmConfig.num_envs_per_env_runner` instead. This will raise an error in the future!\n",
      "2024-05-30 17:35:58,681\tINFO trainable.py:161 -- Trainable.setup took 16.719 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2024-05-30 17:35:58,689\tWARNING util.py:61 -- Install gputil for GPU system monitoring.\n",
      "2024-05-30 17:36:03,397\tWARNING deprecation.py:50 -- DeprecationWarning: `ray.rllib.execution.train_ops.multi_gpu_train_one_step` has been deprecated. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.00454092567617243\n",
      "  StateBufferConnector_ms: 0.0007542696866122159\n",
      "  ViewRequirementAgentConnector_ms: 0.06999752738259056\n",
      "counters:\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-30_17-36-43\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00454092567617243\n",
      "    StateBufferConnector_ms: 0.0007542696866122159\n",
      "    ViewRequirementAgentConnector_ms: 0.06999752738259056\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 22.71590909090909\n",
      "  episode_media: {}\n",
      "  episode_return_max: 69.0\n",
      "  episode_return_mean: 22.71590909090909\n",
      "  episode_return_min: 8.0\n",
      "  episode_reward_max: 69.0\n",
      "  episode_reward_mean: 22.71590909090909\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 176\n",
      "  episodes_timesteps_total: 3998\n",
      "  hist_stats:\n",
      "    episode_lengths: [29, 13, 20, 11, 21, 23, 26, 22, 26, 22, 10, 17, 19, 34, 13,\n",
      "      41, 12, 31, 35, 15, 16, 19, 14, 19, 12, 15, 13, 33, 14, 64, 23, 17, 16, 34,\n",
      "      44, 12, 25, 47, 23, 18, 19, 11, 21, 52, 16, 14, 12, 10, 26, 10, 25, 38, 18,\n",
      "      13, 17, 12, 15, 26, 10, 18, 31, 20, 16, 19, 13, 15, 15, 26, 10, 15, 13, 24,\n",
      "      13, 17, 34, 69, 25, 48, 21, 18, 32, 25, 16, 37, 22, 37, 15, 17, 12, 49, 10,\n",
      "      13, 28, 17, 18, 10, 31, 12, 31, 12, 40, 17, 13, 18, 27, 13, 36, 9, 15, 22, 13,\n",
      "      14, 15, 26, 32, 13, 11, 26, 10, 12, 15, 21, 10, 55, 11, 21, 25, 30, 12, 26,\n",
      "      56, 21, 20, 8, 19, 12, 45, 20, 19, 20, 13, 20, 48, 13, 15, 29, 33, 14, 32, 18,\n",
      "      29, 28, 46, 27, 27, 10, 30, 26, 35, 26, 36, 22, 40, 48, 19, 11, 28, 21, 13,\n",
      "      58, 16, 22, 23, 11, 17, 39]\n",
      "    episode_reward: [29.0, 13.0, 20.0, 11.0, 21.0, 23.0, 26.0, 22.0, 26.0, 22.0, 10.0,\n",
      "      17.0, 19.0, 34.0, 13.0, 41.0, 12.0, 31.0, 35.0, 15.0, 16.0, 19.0, 14.0, 19.0,\n",
      "      12.0, 15.0, 13.0, 33.0, 14.0, 64.0, 23.0, 17.0, 16.0, 34.0, 44.0, 12.0, 25.0,\n",
      "      47.0, 23.0, 18.0, 19.0, 11.0, 21.0, 52.0, 16.0, 14.0, 12.0, 10.0, 26.0, 10.0,\n",
      "      25.0, 38.0, 18.0, 13.0, 17.0, 12.0, 15.0, 26.0, 10.0, 18.0, 31.0, 20.0, 16.0,\n",
      "      19.0, 13.0, 15.0, 15.0, 26.0, 10.0, 15.0, 13.0, 24.0, 13.0, 17.0, 34.0, 69.0,\n",
      "      25.0, 48.0, 21.0, 18.0, 32.0, 25.0, 16.0, 37.0, 22.0, 37.0, 15.0, 17.0, 12.0,\n",
      "      49.0, 10.0, 13.0, 28.0, 17.0, 18.0, 10.0, 31.0, 12.0, 31.0, 12.0, 40.0, 17.0,\n",
      "      13.0, 18.0, 27.0, 13.0, 36.0, 9.0, 15.0, 22.0, 13.0, 14.0, 15.0, 26.0, 32.0,\n",
      "      13.0, 11.0, 26.0, 10.0, 12.0, 15.0, 21.0, 10.0, 55.0, 11.0, 21.0, 25.0, 30.0,\n",
      "      12.0, 26.0, 56.0, 21.0, 20.0, 8.0, 19.0, 12.0, 45.0, 20.0, 19.0, 20.0, 13.0,\n",
      "      20.0, 48.0, 13.0, 15.0, 29.0, 33.0, 14.0, 32.0, 18.0, 29.0, 28.0, 46.0, 27.0,\n",
      "      27.0, 10.0, 30.0, 26.0, 35.0, 26.0, 36.0, 22.0, 40.0, 48.0, 19.0, 11.0, 28.0,\n",
      "      21.0, 13.0, 58.0, 16.0, 22.0, 23.0, 11.0, 17.0, 39.0]\n",
      "  num_episodes: 176\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08767409731048072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024634997447232434\n",
      "    mean_inference_ms: 0.7822309783386369\n",
      "    mean_raw_obs_processing_ms: 0.2600861859005769\n",
      "episode_len_mean: 22.71590909090909\n",
      "episode_media: {}\n",
      "episode_return_max: 69.0\n",
      "episode_return_mean: 22.71590909090909\n",
      "episode_return_min: 8.0\n",
      "episode_reward_max: 69.0\n",
      "episode_reward_mean: 22.71590909090909\n",
      "episode_reward_min: 8.0\n",
      "episodes_this_iter: 176\n",
      "episodes_timesteps_total: 3998\n",
      "episodes_total: 176\n",
      "hostname: LAPTOP-5S3FDGMV\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.20000000000000004\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.6651167688831207\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 1.3398574113204915\n",
      "        kl: 0.027886130381755487\n",
      "        policy_loss: -0.03994180930037332\n",
      "        total_loss: 9.069407668164981\n",
      "        vf_explained_var: -0.05988888400857167\n",
      "        vf_loss: 9.103772240300332\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 465.5\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_env_steps_sampled: 4000\n",
      "  num_env_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 4000\n",
      "num_agent_steps_sampled_lifetime: 4000\n",
      "num_agent_steps_trained: 4000\n",
      "num_env_steps_sampled: 4000\n",
      "num_env_steps_sampled_lifetime: 4000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 90.20355643360128\n",
      "num_env_steps_trained: 4000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 90.20355643360128\n",
      "num_episodes: 176\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 52.61904761904761\n",
      "  ram_util_percent: 58.1174603174603\n",
      "pid: 2612\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08767409731048072\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.024634997447232434\n",
      "  mean_inference_ms: 0.7822309783386369\n",
      "  mean_raw_obs_processing_ms: 0.2600861859005769\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.00454092567617243\n",
      "    StateBufferConnector_ms: 0.0007542696866122159\n",
      "    ViewRequirementAgentConnector_ms: 0.06999752738259056\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 22.71590909090909\n",
      "  episode_media: {}\n",
      "  episode_return_max: 69.0\n",
      "  episode_return_mean: 22.71590909090909\n",
      "  episode_return_min: 8.0\n",
      "  episode_reward_max: 69.0\n",
      "  episode_reward_mean: 22.71590909090909\n",
      "  episode_reward_min: 8.0\n",
      "  episodes_this_iter: 176\n",
      "  episodes_timesteps_total: 3998\n",
      "  hist_stats:\n",
      "    episode_lengths: [29, 13, 20, 11, 21, 23, 26, 22, 26, 22, 10, 17, 19, 34, 13,\n",
      "      41, 12, 31, 35, 15, 16, 19, 14, 19, 12, 15, 13, 33, 14, 64, 23, 17, 16, 34,\n",
      "      44, 12, 25, 47, 23, 18, 19, 11, 21, 52, 16, 14, 12, 10, 26, 10, 25, 38, 18,\n",
      "      13, 17, 12, 15, 26, 10, 18, 31, 20, 16, 19, 13, 15, 15, 26, 10, 15, 13, 24,\n",
      "      13, 17, 34, 69, 25, 48, 21, 18, 32, 25, 16, 37, 22, 37, 15, 17, 12, 49, 10,\n",
      "      13, 28, 17, 18, 10, 31, 12, 31, 12, 40, 17, 13, 18, 27, 13, 36, 9, 15, 22, 13,\n",
      "      14, 15, 26, 32, 13, 11, 26, 10, 12, 15, 21, 10, 55, 11, 21, 25, 30, 12, 26,\n",
      "      56, 21, 20, 8, 19, 12, 45, 20, 19, 20, 13, 20, 48, 13, 15, 29, 33, 14, 32, 18,\n",
      "      29, 28, 46, 27, 27, 10, 30, 26, 35, 26, 36, 22, 40, 48, 19, 11, 28, 21, 13,\n",
      "      58, 16, 22, 23, 11, 17, 39]\n",
      "    episode_reward: [29.0, 13.0, 20.0, 11.0, 21.0, 23.0, 26.0, 22.0, 26.0, 22.0, 10.0,\n",
      "      17.0, 19.0, 34.0, 13.0, 41.0, 12.0, 31.0, 35.0, 15.0, 16.0, 19.0, 14.0, 19.0,\n",
      "      12.0, 15.0, 13.0, 33.0, 14.0, 64.0, 23.0, 17.0, 16.0, 34.0, 44.0, 12.0, 25.0,\n",
      "      47.0, 23.0, 18.0, 19.0, 11.0, 21.0, 52.0, 16.0, 14.0, 12.0, 10.0, 26.0, 10.0,\n",
      "      25.0, 38.0, 18.0, 13.0, 17.0, 12.0, 15.0, 26.0, 10.0, 18.0, 31.0, 20.0, 16.0,\n",
      "      19.0, 13.0, 15.0, 15.0, 26.0, 10.0, 15.0, 13.0, 24.0, 13.0, 17.0, 34.0, 69.0,\n",
      "      25.0, 48.0, 21.0, 18.0, 32.0, 25.0, 16.0, 37.0, 22.0, 37.0, 15.0, 17.0, 12.0,\n",
      "      49.0, 10.0, 13.0, 28.0, 17.0, 18.0, 10.0, 31.0, 12.0, 31.0, 12.0, 40.0, 17.0,\n",
      "      13.0, 18.0, 27.0, 13.0, 36.0, 9.0, 15.0, 22.0, 13.0, 14.0, 15.0, 26.0, 32.0,\n",
      "      13.0, 11.0, 26.0, 10.0, 12.0, 15.0, 21.0, 10.0, 55.0, 11.0, 21.0, 25.0, 30.0,\n",
      "      12.0, 26.0, 56.0, 21.0, 20.0, 8.0, 19.0, 12.0, 45.0, 20.0, 19.0, 20.0, 13.0,\n",
      "      20.0, 48.0, 13.0, 15.0, 29.0, 33.0, 14.0, 32.0, 18.0, 29.0, 28.0, 46.0, 27.0,\n",
      "      27.0, 10.0, 30.0, 26.0, 35.0, 26.0, 36.0, 22.0, 40.0, 48.0, 19.0, 11.0, 28.0,\n",
      "      21.0, 13.0, 58.0, 16.0, 22.0, 23.0, 11.0, 17.0, 39.0]\n",
      "  num_episodes: 176\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08767409731048072\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.024634997447232434\n",
      "    mean_inference_ms: 0.7822309783386369\n",
      "    mean_raw_obs_processing_ms: 0.2600861859005769\n",
      "time_since_restore: 44.352341651916504\n",
      "time_this_iter_s: 44.352341651916504\n",
      "time_total_s: 44.352341651916504\n",
      "timers:\n",
      "  learn_throughput: 100.939\n",
      "  learn_time_ms: 39627.908\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4707.997\n",
      "  synch_weights_time_ms: 8.245\n",
      "  training_iteration_time_ms: 44344.15\n",
      "  training_step_time_ms: 44344.15\n",
      "timestamp: 1717126603\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "trial_id: default\n",
      "\n",
      "Checkpoint saved in directory C:\\Users\\rohit\\AppData\\Local\\Temp\\tmpvrn0m57v\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 8000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0\n",
      "  StateBufferConnector_ms: 0.0\n",
      "  ViewRequirementAgentConnector_ms: 0.0642995834350586\n",
      "counters:\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-30_17-37-27\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.0642995834350586\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.68\n",
      "  episode_media: {}\n",
      "  episode_return_max: 136.0\n",
      "  episode_return_mean: 45.68\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 136.0\n",
      "  episode_reward_mean: 45.68\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 77\n",
      "  episodes_timesteps_total: 4568\n",
      "  hist_stats:\n",
      "    episode_lengths: [27, 27, 10, 30, 26, 35, 26, 36, 22, 40, 48, 19, 11, 28, 21,\n",
      "      13, 58, 16, 22, 23, 11, 17, 39, 12, 21, 58, 25, 24, 34, 16, 22, 75, 61, 108,\n",
      "      101, 41, 69, 58, 37, 31, 21, 54, 17, 15, 49, 62, 14, 79, 99, 67, 38, 17, 45,\n",
      "      93, 118, 131, 116, 60, 99, 43, 51, 17, 16, 104, 39, 25, 40, 56, 32, 93, 131,\n",
      "      21, 26, 87, 68, 23, 78, 13, 35, 78, 17, 17, 44, 17, 70, 28, 28, 24, 48, 14,\n",
      "      17, 59, 73, 114, 36, 136, 92, 16, 25, 25]\n",
      "    episode_reward: [27.0, 27.0, 10.0, 30.0, 26.0, 35.0, 26.0, 36.0, 22.0, 40.0, 48.0,\n",
      "      19.0, 11.0, 28.0, 21.0, 13.0, 58.0, 16.0, 22.0, 23.0, 11.0, 17.0, 39.0, 12.0,\n",
      "      21.0, 58.0, 25.0, 24.0, 34.0, 16.0, 22.0, 75.0, 61.0, 108.0, 101.0, 41.0, 69.0,\n",
      "      58.0, 37.0, 31.0, 21.0, 54.0, 17.0, 15.0, 49.0, 62.0, 14.0, 79.0, 99.0, 67.0,\n",
      "      38.0, 17.0, 45.0, 93.0, 118.0, 131.0, 116.0, 60.0, 99.0, 43.0, 51.0, 17.0, 16.0,\n",
      "      104.0, 39.0, 25.0, 40.0, 56.0, 32.0, 93.0, 131.0, 21.0, 26.0, 87.0, 68.0, 23.0,\n",
      "      78.0, 13.0, 35.0, 78.0, 17.0, 17.0, 44.0, 17.0, 70.0, 28.0, 28.0, 24.0, 48.0,\n",
      "      14.0, 17.0, 59.0, 73.0, 114.0, 36.0, 136.0, 92.0, 16.0, 25.0, 25.0]\n",
      "  num_episodes: 77\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08575346367318579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.031081173747030243\n",
      "    mean_inference_ms: 0.7627799480757937\n",
      "    mean_raw_obs_processing_ms: 0.24010606794467357\n",
      "episode_len_mean: 45.68\n",
      "episode_media: {}\n",
      "episode_return_max: 136.0\n",
      "episode_return_mean: 45.68\n",
      "episode_return_min: 10.0\n",
      "episode_reward_max: 136.0\n",
      "episode_reward_mean: 45.68\n",
      "episode_reward_min: 10.0\n",
      "episodes_this_iter: 77\n",
      "episodes_timesteps_total: 4568\n",
      "episodes_total: 253\n",
      "hostname: LAPTOP-5S3FDGMV\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.6111756914405413\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.67476130629419\n",
      "        kl: 0.01621691396221537\n",
      "        policy_loss: -0.03277058817286004\n",
      "        total_loss: 9.559895361623456\n",
      "        vf_explained_var: 0.01060951537983392\n",
      "        vf_loss: 9.587800860661332\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 1395.5\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_env_steps_sampled: 8000\n",
      "  num_env_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 8000\n",
      "num_agent_steps_sampled_lifetime: 8000\n",
      "num_agent_steps_trained: 8000\n",
      "num_env_steps_sampled: 8000\n",
      "num_env_steps_sampled_lifetime: 8000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 89.35897925567352\n",
      "num_env_steps_trained: 8000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 89.35897925567352\n",
      "num_episodes: 77\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 53.432812500000004\n",
      "  ram_util_percent: 58.153125\n",
      "pid: 2612\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08575346367318579\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.031081173747030243\n",
      "  mean_inference_ms: 0.7627799480757937\n",
      "  mean_raw_obs_processing_ms: 0.24010606794467357\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.0642995834350586\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 45.68\n",
      "  episode_media: {}\n",
      "  episode_return_max: 136.0\n",
      "  episode_return_mean: 45.68\n",
      "  episode_return_min: 10.0\n",
      "  episode_reward_max: 136.0\n",
      "  episode_reward_mean: 45.68\n",
      "  episode_reward_min: 10.0\n",
      "  episodes_this_iter: 77\n",
      "  episodes_timesteps_total: 4568\n",
      "  hist_stats:\n",
      "    episode_lengths: [27, 27, 10, 30, 26, 35, 26, 36, 22, 40, 48, 19, 11, 28, 21,\n",
      "      13, 58, 16, 22, 23, 11, 17, 39, 12, 21, 58, 25, 24, 34, 16, 22, 75, 61, 108,\n",
      "      101, 41, 69, 58, 37, 31, 21, 54, 17, 15, 49, 62, 14, 79, 99, 67, 38, 17, 45,\n",
      "      93, 118, 131, 116, 60, 99, 43, 51, 17, 16, 104, 39, 25, 40, 56, 32, 93, 131,\n",
      "      21, 26, 87, 68, 23, 78, 13, 35, 78, 17, 17, 44, 17, 70, 28, 28, 24, 48, 14,\n",
      "      17, 59, 73, 114, 36, 136, 92, 16, 25, 25]\n",
      "    episode_reward: [27.0, 27.0, 10.0, 30.0, 26.0, 35.0, 26.0, 36.0, 22.0, 40.0, 48.0,\n",
      "      19.0, 11.0, 28.0, 21.0, 13.0, 58.0, 16.0, 22.0, 23.0, 11.0, 17.0, 39.0, 12.0,\n",
      "      21.0, 58.0, 25.0, 24.0, 34.0, 16.0, 22.0, 75.0, 61.0, 108.0, 101.0, 41.0, 69.0,\n",
      "      58.0, 37.0, 31.0, 21.0, 54.0, 17.0, 15.0, 49.0, 62.0, 14.0, 79.0, 99.0, 67.0,\n",
      "      38.0, 17.0, 45.0, 93.0, 118.0, 131.0, 116.0, 60.0, 99.0, 43.0, 51.0, 17.0, 16.0,\n",
      "      104.0, 39.0, 25.0, 40.0, 56.0, 32.0, 93.0, 131.0, 21.0, 26.0, 87.0, 68.0, 23.0,\n",
      "      78.0, 13.0, 35.0, 78.0, 17.0, 17.0, 44.0, 17.0, 70.0, 28.0, 28.0, 24.0, 48.0,\n",
      "      14.0, 17.0, 59.0, 73.0, 114.0, 36.0, 136.0, 92.0, 16.0, 25.0, 25.0]\n",
      "  num_episodes: 77\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08575346367318579\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.031081173747030243\n",
      "    mean_inference_ms: 0.7627799480757937\n",
      "    mean_raw_obs_processing_ms: 0.24010606794467357\n",
      "time_since_restore: 89.12360453605652\n",
      "time_this_iter_s: 44.771262884140015\n",
      "time_total_s: 89.12360453605652\n",
      "timers:\n",
      "  learn_throughput: 99.897\n",
      "  learn_time_ms: 40041.067\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4504.515\n",
      "  synch_weights_time_ms: 8.127\n",
      "  training_iteration_time_ms: 44553.709\n",
      "  training_step_time_ms: 44553.709\n",
      "timestamp: 1717126647\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 12000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0\n",
      "  StateBufferConnector_ms: 0.0\n",
      "  ViewRequirementAgentConnector_ms: 0.08707118034362793\n",
      "counters:\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-30_17-38-12\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.08707118034362793\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 72.3\n",
      "  episode_media: {}\n",
      "  episode_return_max: 203.0\n",
      "  episode_return_mean: 72.3\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 203.0\n",
      "  episode_reward_mean: 72.3\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 39\n",
      "  episodes_timesteps_total: 7230\n",
      "  hist_stats:\n",
      "    episode_lengths: [31, 21, 54, 17, 15, 49, 62, 14, 79, 99, 67, 38, 17, 45, 93,\n",
      "      118, 131, 116, 60, 99, 43, 51, 17, 16, 104, 39, 25, 40, 56, 32, 93, 131, 21,\n",
      "      26, 87, 68, 23, 78, 13, 35, 78, 17, 17, 44, 17, 70, 28, 28, 24, 48, 14, 17,\n",
      "      59, 73, 114, 36, 136, 92, 16, 25, 25, 119, 126, 115, 67, 203, 140, 186, 93,\n",
      "      46, 180, 27, 105, 99, 43, 100, 92, 156, 42, 170, 42, 149, 62, 111, 66, 151,\n",
      "      33, 64, 61, 117, 63, 45, 156, 43, 193, 162, 94, 28, 110, 170]\n",
      "    episode_reward: [31.0, 21.0, 54.0, 17.0, 15.0, 49.0, 62.0, 14.0, 79.0, 99.0, 67.0,\n",
      "      38.0, 17.0, 45.0, 93.0, 118.0, 131.0, 116.0, 60.0, 99.0, 43.0, 51.0, 17.0, 16.0,\n",
      "      104.0, 39.0, 25.0, 40.0, 56.0, 32.0, 93.0, 131.0, 21.0, 26.0, 87.0, 68.0, 23.0,\n",
      "      78.0, 13.0, 35.0, 78.0, 17.0, 17.0, 44.0, 17.0, 70.0, 28.0, 28.0, 24.0, 48.0,\n",
      "      14.0, 17.0, 59.0, 73.0, 114.0, 36.0, 136.0, 92.0, 16.0, 25.0, 25.0, 119.0, 126.0,\n",
      "      115.0, 67.0, 203.0, 140.0, 186.0, 93.0, 46.0, 180.0, 27.0, 105.0, 99.0, 43.0,\n",
      "      100.0, 92.0, 156.0, 42.0, 170.0, 42.0, 149.0, 62.0, 111.0, 66.0, 151.0, 33.0,\n",
      "      64.0, 61.0, 117.0, 63.0, 45.0, 156.0, 43.0, 193.0, 162.0, 94.0, 28.0, 110.0,\n",
      "      170.0]\n",
      "  num_episodes: 39\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08624397037960042\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.031144744858917238\n",
      "    mean_inference_ms: 0.7641157138949913\n",
      "    mean_raw_obs_processing_ms: 0.2329859641601334\n",
      "episode_len_mean: 72.3\n",
      "episode_media: {}\n",
      "episode_return_max: 203.0\n",
      "episode_return_mean: 72.3\n",
      "episode_return_min: 13.0\n",
      "episode_reward_max: 203.0\n",
      "episode_reward_mean: 72.3\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 39\n",
      "episodes_timesteps_total: 7230\n",
      "episodes_total: 292\n",
      "hostname: LAPTOP-5S3FDGMV\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5728821316713928\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.6071152316145999\n",
      "        kl: 0.010270594976751816\n",
      "        policy_loss: -0.027958715988463292\n",
      "        total_loss: 9.690224098902876\n",
      "        vf_explained_var: 0.005494559003460792\n",
      "        vf_loss: 9.715101633789718\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 2325.5\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_env_steps_sampled: 12000\n",
      "  num_env_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 12000\n",
      "num_agent_steps_sampled_lifetime: 12000\n",
      "num_agent_steps_trained: 12000\n",
      "num_env_steps_sampled: 12000\n",
      "num_env_steps_sampled_lifetime: 12000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 89.62450444395758\n",
      "num_env_steps_trained: 12000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 89.62450444395758\n",
      "num_episodes: 39\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 52.41111111111111\n",
      "  ram_util_percent: 56.53174603174602\n",
      "pid: 2612\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08624397037960042\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.031144744858917238\n",
      "  mean_inference_ms: 0.7641157138949913\n",
      "  mean_raw_obs_processing_ms: 0.2329859641601334\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.08707118034362793\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 72.3\n",
      "  episode_media: {}\n",
      "  episode_return_max: 203.0\n",
      "  episode_return_mean: 72.3\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 203.0\n",
      "  episode_reward_mean: 72.3\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 39\n",
      "  episodes_timesteps_total: 7230\n",
      "  hist_stats:\n",
      "    episode_lengths: [31, 21, 54, 17, 15, 49, 62, 14, 79, 99, 67, 38, 17, 45, 93,\n",
      "      118, 131, 116, 60, 99, 43, 51, 17, 16, 104, 39, 25, 40, 56, 32, 93, 131, 21,\n",
      "      26, 87, 68, 23, 78, 13, 35, 78, 17, 17, 44, 17, 70, 28, 28, 24, 48, 14, 17,\n",
      "      59, 73, 114, 36, 136, 92, 16, 25, 25, 119, 126, 115, 67, 203, 140, 186, 93,\n",
      "      46, 180, 27, 105, 99, 43, 100, 92, 156, 42, 170, 42, 149, 62, 111, 66, 151,\n",
      "      33, 64, 61, 117, 63, 45, 156, 43, 193, 162, 94, 28, 110, 170]\n",
      "    episode_reward: [31.0, 21.0, 54.0, 17.0, 15.0, 49.0, 62.0, 14.0, 79.0, 99.0, 67.0,\n",
      "      38.0, 17.0, 45.0, 93.0, 118.0, 131.0, 116.0, 60.0, 99.0, 43.0, 51.0, 17.0, 16.0,\n",
      "      104.0, 39.0, 25.0, 40.0, 56.0, 32.0, 93.0, 131.0, 21.0, 26.0, 87.0, 68.0, 23.0,\n",
      "      78.0, 13.0, 35.0, 78.0, 17.0, 17.0, 44.0, 17.0, 70.0, 28.0, 28.0, 24.0, 48.0,\n",
      "      14.0, 17.0, 59.0, 73.0, 114.0, 36.0, 136.0, 92.0, 16.0, 25.0, 25.0, 119.0, 126.0,\n",
      "      115.0, 67.0, 203.0, 140.0, 186.0, 93.0, 46.0, 180.0, 27.0, 105.0, 99.0, 43.0,\n",
      "      100.0, 92.0, 156.0, 42.0, 170.0, 42.0, 149.0, 62.0, 111.0, 66.0, 151.0, 33.0,\n",
      "      64.0, 61.0, 117.0, 63.0, 45.0, 156.0, 43.0, 193.0, 162.0, 94.0, 28.0, 110.0,\n",
      "      170.0]\n",
      "  num_episodes: 39\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08624397037960042\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.031144744858917238\n",
      "    mean_inference_ms: 0.7641157138949913\n",
      "    mean_raw_obs_processing_ms: 0.2329859641601334\n",
      "time_since_restore: 133.75425577163696\n",
      "time_this_iter_s: 44.630651235580444\n",
      "time_total_s: 133.75425577163696\n",
      "timers:\n",
      "  learn_throughput: 99.938\n",
      "  learn_time_ms: 40024.9\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4546.369\n",
      "  synch_weights_time_ms: 8.087\n",
      "  training_iteration_time_ms: 44579.357\n",
      "  training_step_time_ms: 44579.357\n",
      "timestamp: 1717126692\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 16000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0\n",
      "  StateBufferConnector_ms: 0.0\n",
      "  ViewRequirementAgentConnector_ms: 0.0968019962310791\n",
      "counters:\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-30_17-38-55\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.0968019962310791\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 97.63\n",
      "  episode_media: {}\n",
      "  episode_return_max: 346.0\n",
      "  episode_return_mean: 97.63\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 346.0\n",
      "  episode_reward_mean: 97.63\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_timesteps_total: 9763\n",
      "  hist_stats:\n",
      "    episode_lengths: [16, 104, 39, 25, 40, 56, 32, 93, 131, 21, 26, 87, 68, 23, 78,\n",
      "      13, 35, 78, 17, 17, 44, 17, 70, 28, 28, 24, 48, 14, 17, 59, 73, 114, 36, 136,\n",
      "      92, 16, 25, 25, 119, 126, 115, 67, 203, 140, 186, 93, 46, 180, 27, 105, 99,\n",
      "      43, 100, 92, 156, 42, 170, 42, 149, 62, 111, 66, 151, 33, 64, 61, 117, 63, 45,\n",
      "      156, 43, 193, 162, 94, 28, 110, 170, 230, 198, 170, 174, 142, 13, 263, 170,\n",
      "      195, 135, 131, 127, 151, 86, 242, 168, 150, 260, 346, 61, 72, 186, 199]\n",
      "    episode_reward: [16.0, 104.0, 39.0, 25.0, 40.0, 56.0, 32.0, 93.0, 131.0, 21.0,\n",
      "      26.0, 87.0, 68.0, 23.0, 78.0, 13.0, 35.0, 78.0, 17.0, 17.0, 44.0, 17.0, 70.0,\n",
      "      28.0, 28.0, 24.0, 48.0, 14.0, 17.0, 59.0, 73.0, 114.0, 36.0, 136.0, 92.0, 16.0,\n",
      "      25.0, 25.0, 119.0, 126.0, 115.0, 67.0, 203.0, 140.0, 186.0, 93.0, 46.0, 180.0,\n",
      "      27.0, 105.0, 99.0, 43.0, 100.0, 92.0, 156.0, 42.0, 170.0, 42.0, 149.0, 62.0,\n",
      "      111.0, 66.0, 151.0, 33.0, 64.0, 61.0, 117.0, 63.0, 45.0, 156.0, 43.0, 193.0,\n",
      "      162.0, 94.0, 28.0, 110.0, 170.0, 230.0, 198.0, 170.0, 174.0, 142.0, 13.0, 263.0,\n",
      "      170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0, 242.0, 168.0, 150.0, 260.0,\n",
      "      346.0, 61.0, 72.0, 186.0, 199.0]\n",
      "  num_episodes: 23\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08653204251566543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0296602464419123\n",
      "    mean_inference_ms: 0.7644929500375867\n",
      "    mean_raw_obs_processing_ms: 0.23160979353426636\n",
      "episode_len_mean: 97.63\n",
      "episode_media: {}\n",
      "episode_return_max: 346.0\n",
      "episode_return_mean: 97.63\n",
      "episode_return_min: 13.0\n",
      "episode_reward_max: 346.0\n",
      "episode_reward_mean: 97.63\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 23\n",
      "episodes_timesteps_total: 9763\n",
      "episodes_total: 315\n",
      "hostname: LAPTOP-5S3FDGMV\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5743946891318086\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.5874819806226159\n",
      "        kl: 0.007403096939042591\n",
      "        policy_loss: -0.019694429402670233\n",
      "        total_loss: 9.775435232347057\n",
      "        vf_explained_var: 0.051600247301081176\n",
      "        vf_loss: 9.792908735172723\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 3255.5\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_env_steps_sampled: 16000\n",
      "  num_env_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 16000\n",
      "num_agent_steps_sampled_lifetime: 16000\n",
      "num_agent_steps_trained: 16000\n",
      "num_env_steps_sampled: 16000\n",
      "num_env_steps_sampled_lifetime: 16000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 93.09173347837486\n",
      "num_env_steps_trained: 16000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 93.09173347837486\n",
      "num_episodes: 23\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 49.72295081967212\n",
      "  ram_util_percent: 55.52131147540984\n",
      "pid: 2612\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08653204251566543\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.0296602464419123\n",
      "  mean_inference_ms: 0.7644929500375867\n",
      "  mean_raw_obs_processing_ms: 0.23160979353426636\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.0968019962310791\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 97.63\n",
      "  episode_media: {}\n",
      "  episode_return_max: 346.0\n",
      "  episode_return_mean: 97.63\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 346.0\n",
      "  episode_reward_mean: 97.63\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 23\n",
      "  episodes_timesteps_total: 9763\n",
      "  hist_stats:\n",
      "    episode_lengths: [16, 104, 39, 25, 40, 56, 32, 93, 131, 21, 26, 87, 68, 23, 78,\n",
      "      13, 35, 78, 17, 17, 44, 17, 70, 28, 28, 24, 48, 14, 17, 59, 73, 114, 36, 136,\n",
      "      92, 16, 25, 25, 119, 126, 115, 67, 203, 140, 186, 93, 46, 180, 27, 105, 99,\n",
      "      43, 100, 92, 156, 42, 170, 42, 149, 62, 111, 66, 151, 33, 64, 61, 117, 63, 45,\n",
      "      156, 43, 193, 162, 94, 28, 110, 170, 230, 198, 170, 174, 142, 13, 263, 170,\n",
      "      195, 135, 131, 127, 151, 86, 242, 168, 150, 260, 346, 61, 72, 186, 199]\n",
      "    episode_reward: [16.0, 104.0, 39.0, 25.0, 40.0, 56.0, 32.0, 93.0, 131.0, 21.0,\n",
      "      26.0, 87.0, 68.0, 23.0, 78.0, 13.0, 35.0, 78.0, 17.0, 17.0, 44.0, 17.0, 70.0,\n",
      "      28.0, 28.0, 24.0, 48.0, 14.0, 17.0, 59.0, 73.0, 114.0, 36.0, 136.0, 92.0, 16.0,\n",
      "      25.0, 25.0, 119.0, 126.0, 115.0, 67.0, 203.0, 140.0, 186.0, 93.0, 46.0, 180.0,\n",
      "      27.0, 105.0, 99.0, 43.0, 100.0, 92.0, 156.0, 42.0, 170.0, 42.0, 149.0, 62.0,\n",
      "      111.0, 66.0, 151.0, 33.0, 64.0, 61.0, 117.0, 63.0, 45.0, 156.0, 43.0, 193.0,\n",
      "      162.0, 94.0, 28.0, 110.0, 170.0, 230.0, 198.0, 170.0, 174.0, 142.0, 13.0, 263.0,\n",
      "      170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0, 242.0, 168.0, 150.0, 260.0,\n",
      "      346.0, 61.0, 72.0, 186.0, 199.0]\n",
      "  num_episodes: 23\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08653204251566543\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0296602464419123\n",
      "    mean_inference_ms: 0.7644929500375867\n",
      "    mean_raw_obs_processing_ms: 0.23160979353426636\n",
      "time_since_restore: 176.7307426929474\n",
      "time_this_iter_s: 42.976486921310425\n",
      "time_total_s: 176.7307426929474\n",
      "timers:\n",
      "  learn_throughput: 100.702\n",
      "  learn_time_ms: 39721.182\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4448.215\n",
      "  synch_weights_time_ms: 7.212\n",
      "  training_iteration_time_ms: 44176.61\n",
      "  training_step_time_ms: 44176.61\n",
      "timestamp: 1717126735\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 20000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0\n",
      "  StateBufferConnector_ms: 0.0\n",
      "  ViewRequirementAgentConnector_ms: 0.13031554222106934\n",
      "counters:\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-30_17-39-38\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.13031554222106934\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 128.64\n",
      "  episode_media: {}\n",
      "  episode_return_max: 390.0\n",
      "  episode_return_mean: 128.64\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 390.0\n",
      "  episode_reward_mean: 128.64\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_timesteps_total: 12864\n",
      "  hist_stats:\n",
      "    episode_lengths: [17, 17, 44, 17, 70, 28, 28, 24, 48, 14, 17, 59, 73, 114, 36,\n",
      "      136, 92, 16, 25, 25, 119, 126, 115, 67, 203, 140, 186, 93, 46, 180, 27, 105,\n",
      "      99, 43, 100, 92, 156, 42, 170, 42, 149, 62, 111, 66, 151, 33, 64, 61, 117, 63,\n",
      "      45, 156, 43, 193, 162, 94, 28, 110, 170, 230, 198, 170, 174, 142, 13, 263, 170,\n",
      "      195, 135, 131, 127, 151, 86, 242, 168, 150, 260, 346, 61, 72, 186, 199, 309,\n",
      "      200, 217, 207, 150, 220, 390, 281, 196, 215, 168, 252, 203, 212, 123, 163, 385,\n",
      "      175]\n",
      "    episode_reward: [17.0, 17.0, 44.0, 17.0, 70.0, 28.0, 28.0, 24.0, 48.0, 14.0, 17.0,\n",
      "      59.0, 73.0, 114.0, 36.0, 136.0, 92.0, 16.0, 25.0, 25.0, 119.0, 126.0, 115.0,\n",
      "      67.0, 203.0, 140.0, 186.0, 93.0, 46.0, 180.0, 27.0, 105.0, 99.0, 43.0, 100.0,\n",
      "      92.0, 156.0, 42.0, 170.0, 42.0, 149.0, 62.0, 111.0, 66.0, 151.0, 33.0, 64.0,\n",
      "      61.0, 117.0, 63.0, 45.0, 156.0, 43.0, 193.0, 162.0, 94.0, 28.0, 110.0, 170.0,\n",
      "      230.0, 198.0, 170.0, 174.0, 142.0, 13.0, 263.0, 170.0, 195.0, 135.0, 131.0,\n",
      "      127.0, 151.0, 86.0, 242.0, 168.0, 150.0, 260.0, 346.0, 61.0, 72.0, 186.0, 199.0,\n",
      "      309.0, 200.0, 217.0, 207.0, 150.0, 220.0, 390.0, 281.0, 196.0, 215.0, 168.0,\n",
      "      252.0, 203.0, 212.0, 123.0, 163.0, 385.0, 175.0]\n",
      "  num_episodes: 18\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0862358318124699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.028570825505181724\n",
      "    mean_inference_ms: 0.7651871063052903\n",
      "    mean_raw_obs_processing_ms: 0.23039334933424505\n",
      "episode_len_mean: 128.64\n",
      "episode_media: {}\n",
      "episode_return_max: 390.0\n",
      "episode_return_mean: 128.64\n",
      "episode_return_min: 13.0\n",
      "episode_reward_max: 390.0\n",
      "episode_reward_mean: 128.64\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 18\n",
      "episodes_timesteps_total: 12864\n",
      "episodes_total: 333\n",
      "hostname: LAPTOP-5S3FDGMV\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5487392851742365\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.4782726565235725\n",
      "        kl: 0.005508720726397182\n",
      "        policy_loss: -0.020051014611637722\n",
      "        total_loss: 9.788597327406688\n",
      "        vf_explained_var: -0.01033397695069672\n",
      "        vf_loss: 9.806995749729936\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 4185.5\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_env_steps_sampled: 20000\n",
      "  num_env_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 20000\n",
      "num_agent_steps_sampled_lifetime: 20000\n",
      "num_agent_steps_trained: 20000\n",
      "num_env_steps_sampled: 20000\n",
      "num_env_steps_sampled_lifetime: 20000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 93.47075799942304\n",
      "num_env_steps_trained: 20000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 93.47075799942304\n",
      "num_episodes: 18\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 51.061666666666675\n",
      "  ram_util_percent: 54.74999999999999\n",
      "pid: 2612\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0862358318124699\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.028570825505181724\n",
      "  mean_inference_ms: 0.7651871063052903\n",
      "  mean_raw_obs_processing_ms: 0.23039334933424505\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.13031554222106934\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 128.64\n",
      "  episode_media: {}\n",
      "  episode_return_max: 390.0\n",
      "  episode_return_mean: 128.64\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 390.0\n",
      "  episode_reward_mean: 128.64\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 18\n",
      "  episodes_timesteps_total: 12864\n",
      "  hist_stats:\n",
      "    episode_lengths: [17, 17, 44, 17, 70, 28, 28, 24, 48, 14, 17, 59, 73, 114, 36,\n",
      "      136, 92, 16, 25, 25, 119, 126, 115, 67, 203, 140, 186, 93, 46, 180, 27, 105,\n",
      "      99, 43, 100, 92, 156, 42, 170, 42, 149, 62, 111, 66, 151, 33, 64, 61, 117, 63,\n",
      "      45, 156, 43, 193, 162, 94, 28, 110, 170, 230, 198, 170, 174, 142, 13, 263, 170,\n",
      "      195, 135, 131, 127, 151, 86, 242, 168, 150, 260, 346, 61, 72, 186, 199, 309,\n",
      "      200, 217, 207, 150, 220, 390, 281, 196, 215, 168, 252, 203, 212, 123, 163, 385,\n",
      "      175]\n",
      "    episode_reward: [17.0, 17.0, 44.0, 17.0, 70.0, 28.0, 28.0, 24.0, 48.0, 14.0, 17.0,\n",
      "      59.0, 73.0, 114.0, 36.0, 136.0, 92.0, 16.0, 25.0, 25.0, 119.0, 126.0, 115.0,\n",
      "      67.0, 203.0, 140.0, 186.0, 93.0, 46.0, 180.0, 27.0, 105.0, 99.0, 43.0, 100.0,\n",
      "      92.0, 156.0, 42.0, 170.0, 42.0, 149.0, 62.0, 111.0, 66.0, 151.0, 33.0, 64.0,\n",
      "      61.0, 117.0, 63.0, 45.0, 156.0, 43.0, 193.0, 162.0, 94.0, 28.0, 110.0, 170.0,\n",
      "      230.0, 198.0, 170.0, 174.0, 142.0, 13.0, 263.0, 170.0, 195.0, 135.0, 131.0,\n",
      "      127.0, 151.0, 86.0, 242.0, 168.0, 150.0, 260.0, 346.0, 61.0, 72.0, 186.0, 199.0,\n",
      "      309.0, 200.0, 217.0, 207.0, 150.0, 220.0, 390.0, 281.0, 196.0, 215.0, 168.0,\n",
      "      252.0, 203.0, 212.0, 123.0, 163.0, 385.0, 175.0]\n",
      "  num_episodes: 18\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0862358318124699\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.028570825505181724\n",
      "    mean_inference_ms: 0.7651871063052903\n",
      "    mean_raw_obs_processing_ms: 0.23039334933424505\n",
      "time_since_restore: 219.53287434577942\n",
      "time_this_iter_s: 42.80213165283203\n",
      "time_total_s: 219.53287434577942\n",
      "timers:\n",
      "  learn_throughput: 101.393\n",
      "  learn_time_ms: 39450.521\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4442.201\n",
      "  synch_weights_time_ms: 7.392\n",
      "  training_iteration_time_ms: 43900.114\n",
      "  training_step_time_ms: 43900.114\n",
      "timestamp: 1717126778\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 24000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0\n",
      "  StateBufferConnector_ms: 0.0\n",
      "  ViewRequirementAgentConnector_ms: 0.13881659507751465\n",
      "counters:\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-30_17-40-22\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.13881659507751465\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 161.68\n",
      "  episode_media: {}\n",
      "  episode_return_max: 439.0\n",
      "  episode_return_mean: 161.68\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 439.0\n",
      "  episode_reward_mean: 161.68\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_timesteps_total: 16168\n",
      "  hist_stats:\n",
      "    episode_lengths: [92, 16, 25, 25, 119, 126, 115, 67, 203, 140, 186, 93, 46, 180,\n",
      "      27, 105, 99, 43, 100, 92, 156, 42, 170, 42, 149, 62, 111, 66, 151, 33, 64, 61,\n",
      "      117, 63, 45, 156, 43, 193, 162, 94, 28, 110, 170, 230, 198, 170, 174, 142, 13,\n",
      "      263, 170, 195, 135, 131, 127, 151, 86, 242, 168, 150, 260, 346, 61, 72, 186,\n",
      "      199, 309, 200, 217, 207, 150, 220, 390, 281, 196, 215, 168, 252, 203, 212, 123,\n",
      "      163, 385, 175, 197, 179, 237, 254, 233, 205, 228, 236, 233, 370, 275, 174, 439,\n",
      "      267, 279, 240]\n",
      "    episode_reward: [92.0, 16.0, 25.0, 25.0, 119.0, 126.0, 115.0, 67.0, 203.0, 140.0,\n",
      "      186.0, 93.0, 46.0, 180.0, 27.0, 105.0, 99.0, 43.0, 100.0, 92.0, 156.0, 42.0,\n",
      "      170.0, 42.0, 149.0, 62.0, 111.0, 66.0, 151.0, 33.0, 64.0, 61.0, 117.0, 63.0,\n",
      "      45.0, 156.0, 43.0, 193.0, 162.0, 94.0, 28.0, 110.0, 170.0, 230.0, 198.0, 170.0,\n",
      "      174.0, 142.0, 13.0, 263.0, 170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0, 242.0,\n",
      "      168.0, 150.0, 260.0, 346.0, 61.0, 72.0, 186.0, 199.0, 309.0, 200.0, 217.0, 207.0,\n",
      "      150.0, 220.0, 390.0, 281.0, 196.0, 215.0, 168.0, 252.0, 203.0, 212.0, 123.0,\n",
      "      163.0, 385.0, 175.0, 197.0, 179.0, 237.0, 254.0, 233.0, 205.0, 228.0, 236.0,\n",
      "      233.0, 370.0, 275.0, 174.0, 439.0, 267.0, 279.0, 240.0]\n",
      "  num_episodes: 16\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08591967578677895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02780193359197465\n",
      "    mean_inference_ms: 0.7650339095567483\n",
      "    mean_raw_obs_processing_ms: 0.22790745299803844\n",
      "episode_len_mean: 161.68\n",
      "episode_media: {}\n",
      "episode_return_max: 439.0\n",
      "episode_return_mean: 161.68\n",
      "episode_return_min: 13.0\n",
      "episode_reward_max: 439.0\n",
      "episode_reward_mean: 161.68\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 16\n",
      "episodes_timesteps_total: 16168\n",
      "episodes_total: 349\n",
      "hostname: LAPTOP-5S3FDGMV\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.3\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5728573904883477\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.5049512859954629\n",
      "        kl: 0.002138324025825065\n",
      "        policy_loss: -0.01697063746671843\n",
      "        total_loss: 9.793137983096543\n",
      "        vf_explained_var: -0.0027391370906624745\n",
      "        vf_loss: 9.80946711570986\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 5115.5\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_env_steps_sampled: 24000\n",
      "  num_env_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 24000\n",
      "num_agent_steps_sampled_lifetime: 24000\n",
      "num_agent_steps_trained: 24000\n",
      "num_env_steps_sampled: 24000\n",
      "num_env_steps_sampled_lifetime: 24000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 91.01512450412531\n",
      "num_env_steps_trained: 24000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 91.01512450412531\n",
      "num_episodes: 16\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 51.98225806451614\n",
      "  ram_util_percent: 54.9209677419355\n",
      "pid: 2612\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08591967578677895\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02780193359197465\n",
      "  mean_inference_ms: 0.7650339095567483\n",
      "  mean_raw_obs_processing_ms: 0.22790745299803844\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.13881659507751465\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 161.68\n",
      "  episode_media: {}\n",
      "  episode_return_max: 439.0\n",
      "  episode_return_mean: 161.68\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 439.0\n",
      "  episode_reward_mean: 161.68\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 16\n",
      "  episodes_timesteps_total: 16168\n",
      "  hist_stats:\n",
      "    episode_lengths: [92, 16, 25, 25, 119, 126, 115, 67, 203, 140, 186, 93, 46, 180,\n",
      "      27, 105, 99, 43, 100, 92, 156, 42, 170, 42, 149, 62, 111, 66, 151, 33, 64, 61,\n",
      "      117, 63, 45, 156, 43, 193, 162, 94, 28, 110, 170, 230, 198, 170, 174, 142, 13,\n",
      "      263, 170, 195, 135, 131, 127, 151, 86, 242, 168, 150, 260, 346, 61, 72, 186,\n",
      "      199, 309, 200, 217, 207, 150, 220, 390, 281, 196, 215, 168, 252, 203, 212, 123,\n",
      "      163, 385, 175, 197, 179, 237, 254, 233, 205, 228, 236, 233, 370, 275, 174, 439,\n",
      "      267, 279, 240]\n",
      "    episode_reward: [92.0, 16.0, 25.0, 25.0, 119.0, 126.0, 115.0, 67.0, 203.0, 140.0,\n",
      "      186.0, 93.0, 46.0, 180.0, 27.0, 105.0, 99.0, 43.0, 100.0, 92.0, 156.0, 42.0,\n",
      "      170.0, 42.0, 149.0, 62.0, 111.0, 66.0, 151.0, 33.0, 64.0, 61.0, 117.0, 63.0,\n",
      "      45.0, 156.0, 43.0, 193.0, 162.0, 94.0, 28.0, 110.0, 170.0, 230.0, 198.0, 170.0,\n",
      "      174.0, 142.0, 13.0, 263.0, 170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0, 242.0,\n",
      "      168.0, 150.0, 260.0, 346.0, 61.0, 72.0, 186.0, 199.0, 309.0, 200.0, 217.0, 207.0,\n",
      "      150.0, 220.0, 390.0, 281.0, 196.0, 215.0, 168.0, 252.0, 203.0, 212.0, 123.0,\n",
      "      163.0, 385.0, 175.0, 197.0, 179.0, 237.0, 254.0, 233.0, 205.0, 228.0, 236.0,\n",
      "      233.0, 370.0, 275.0, 174.0, 439.0, 267.0, 279.0, 240.0]\n",
      "  num_episodes: 16\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08591967578677895\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02780193359197465\n",
      "    mean_inference_ms: 0.7650339095567483\n",
      "    mean_raw_obs_processing_ms: 0.22790745299803844\n",
      "time_since_restore: 263.48961305618286\n",
      "time_this_iter_s: 43.95673871040344\n",
      "time_total_s: 263.48961305618286\n",
      "timers:\n",
      "  learn_throughput: 101.242\n",
      "  learn_time_ms: 39509.453\n",
      "  load_throughput: 0.0\n",
      "  load_time_ms: 0.0\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4392.605\n",
      "  synch_weights_time_ms: 6.16\n",
      "  training_iteration_time_ms: 43908.219\n",
      "  training_step_time_ms: 43908.219\n",
      "timestamp: 1717126822\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "trial_id: default\n",
      "\n",
      "Checkpoint saved in directory C:\\Users\\rohit\\AppData\\Local\\Temp\\tmpf4xszrho\n",
      "agent_timesteps_total: 28000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0\n",
      "  StateBufferConnector_ms: 0.0\n",
      "  ViewRequirementAgentConnector_ms: 0.15674161911010742\n",
      "counters:\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-30_17-41-07\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.15674161911010742\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 185.56\n",
      "  episode_media: {}\n",
      "  episode_return_max: 439.0\n",
      "  episode_return_mean: 185.56\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 439.0\n",
      "  episode_reward_mean: 185.56\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_timesteps_total: 18556\n",
      "  hist_stats:\n",
      "    episode_lengths: [105, 99, 43, 100, 92, 156, 42, 170, 42, 149, 62, 111, 66, 151,\n",
      "      33, 64, 61, 117, 63, 45, 156, 43, 193, 162, 94, 28, 110, 170, 230, 198, 170,\n",
      "      174, 142, 13, 263, 170, 195, 135, 131, 127, 151, 86, 242, 168, 150, 260, 346,\n",
      "      61, 72, 186, 199, 309, 200, 217, 207, 150, 220, 390, 281, 196, 215, 168, 252,\n",
      "      203, 212, 123, 163, 385, 175, 197, 179, 237, 254, 233, 205, 228, 236, 233, 370,\n",
      "      275, 174, 439, 267, 279, 240, 83, 395, 288, 218, 197, 231, 271, 211, 258, 355,\n",
      "      240, 346, 206, 268, 281]\n",
      "    episode_reward: [105.0, 99.0, 43.0, 100.0, 92.0, 156.0, 42.0, 170.0, 42.0, 149.0,\n",
      "      62.0, 111.0, 66.0, 151.0, 33.0, 64.0, 61.0, 117.0, 63.0, 45.0, 156.0, 43.0,\n",
      "      193.0, 162.0, 94.0, 28.0, 110.0, 170.0, 230.0, 198.0, 170.0, 174.0, 142.0, 13.0,\n",
      "      263.0, 170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0, 242.0, 168.0, 150.0,\n",
      "      260.0, 346.0, 61.0, 72.0, 186.0, 199.0, 309.0, 200.0, 217.0, 207.0, 150.0, 220.0,\n",
      "      390.0, 281.0, 196.0, 215.0, 168.0, 252.0, 203.0, 212.0, 123.0, 163.0, 385.0,\n",
      "      175.0, 197.0, 179.0, 237.0, 254.0, 233.0, 205.0, 228.0, 236.0, 233.0, 370.0,\n",
      "      275.0, 174.0, 439.0, 267.0, 279.0, 240.0, 83.0, 395.0, 288.0, 218.0, 197.0,\n",
      "      231.0, 271.0, 211.0, 258.0, 355.0, 240.0, 346.0, 206.0, 268.0, 281.0]\n",
      "  num_episodes: 15\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0856445698311788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.027719612200124644\n",
      "    mean_inference_ms: 0.7635053681664652\n",
      "    mean_raw_obs_processing_ms: 0.2257027783845249\n",
      "episode_len_mean: 185.56\n",
      "episode_media: {}\n",
      "episode_return_max: 439.0\n",
      "episode_return_mean: 185.56\n",
      "episode_return_min: 13.0\n",
      "episode_reward_max: 439.0\n",
      "episode_reward_mean: 185.56\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 15\n",
      "episodes_timesteps_total: 18556\n",
      "episodes_total: 364\n",
      "hostname: LAPTOP-5S3FDGMV\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.15\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5542588758853174\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.5379868237762362\n",
      "        kl: 0.0022529902929971295\n",
      "        policy_loss: -0.015175576049393864\n",
      "        total_loss: 9.78156312306722\n",
      "        vf_explained_var: 0.0514252955554634\n",
      "        vf_loss: 9.796400776729788\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 6045.5\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_env_steps_sampled: 28000\n",
      "  num_env_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 28000\n",
      "num_agent_steps_sampled_lifetime: 28000\n",
      "num_agent_steps_trained: 28000\n",
      "num_env_steps_sampled: 28000\n",
      "num_env_steps_sampled_lifetime: 28000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 89.1981197063655\n",
      "num_env_steps_trained: 28000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 89.1981197063655\n",
      "num_episodes: 15\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 52.39375\n",
      "  ram_util_percent: 55.0546875\n",
      "pid: 2612\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.0856445698311788\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.027719612200124644\n",
      "  mean_inference_ms: 0.7635053681664652\n",
      "  mean_raw_obs_processing_ms: 0.2257027783845249\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.15674161911010742\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 185.56\n",
      "  episode_media: {}\n",
      "  episode_return_max: 439.0\n",
      "  episode_return_mean: 185.56\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 439.0\n",
      "  episode_reward_mean: 185.56\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 15\n",
      "  episodes_timesteps_total: 18556\n",
      "  hist_stats:\n",
      "    episode_lengths: [105, 99, 43, 100, 92, 156, 42, 170, 42, 149, 62, 111, 66, 151,\n",
      "      33, 64, 61, 117, 63, 45, 156, 43, 193, 162, 94, 28, 110, 170, 230, 198, 170,\n",
      "      174, 142, 13, 263, 170, 195, 135, 131, 127, 151, 86, 242, 168, 150, 260, 346,\n",
      "      61, 72, 186, 199, 309, 200, 217, 207, 150, 220, 390, 281, 196, 215, 168, 252,\n",
      "      203, 212, 123, 163, 385, 175, 197, 179, 237, 254, 233, 205, 228, 236, 233, 370,\n",
      "      275, 174, 439, 267, 279, 240, 83, 395, 288, 218, 197, 231, 271, 211, 258, 355,\n",
      "      240, 346, 206, 268, 281]\n",
      "    episode_reward: [105.0, 99.0, 43.0, 100.0, 92.0, 156.0, 42.0, 170.0, 42.0, 149.0,\n",
      "      62.0, 111.0, 66.0, 151.0, 33.0, 64.0, 61.0, 117.0, 63.0, 45.0, 156.0, 43.0,\n",
      "      193.0, 162.0, 94.0, 28.0, 110.0, 170.0, 230.0, 198.0, 170.0, 174.0, 142.0, 13.0,\n",
      "      263.0, 170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0, 242.0, 168.0, 150.0,\n",
      "      260.0, 346.0, 61.0, 72.0, 186.0, 199.0, 309.0, 200.0, 217.0, 207.0, 150.0, 220.0,\n",
      "      390.0, 281.0, 196.0, 215.0, 168.0, 252.0, 203.0, 212.0, 123.0, 163.0, 385.0,\n",
      "      175.0, 197.0, 179.0, 237.0, 254.0, 233.0, 205.0, 228.0, 236.0, 233.0, 370.0,\n",
      "      275.0, 174.0, 439.0, 267.0, 279.0, 240.0, 83.0, 395.0, 288.0, 218.0, 197.0,\n",
      "      231.0, 271.0, 211.0, 258.0, 355.0, 240.0, 346.0, 206.0, 268.0, 281.0]\n",
      "  num_episodes: 15\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0856445698311788\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.027719612200124644\n",
      "    mean_inference_ms: 0.7635053681664652\n",
      "    mean_raw_obs_processing_ms: 0.2257027783845249\n",
      "time_since_restore: 308.3364269733429\n",
      "time_this_iter_s: 44.846813917160034\n",
      "time_total_s: 308.3364269733429\n",
      "timers:\n",
      "  learn_throughput: 100.958\n",
      "  learn_time_ms: 39620.293\n",
      "  load_throughput: 27060025.806\n",
      "  load_time_ms: 0.148\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4415.045\n",
      "  synch_weights_time_ms: 6.415\n",
      "  training_iteration_time_ms: 44042.304\n",
      "  training_step_time_ms: 44041.901\n",
      "timestamp: 1717126867\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 32000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0\n",
      "  StateBufferConnector_ms: 0.0\n",
      "  ViewRequirementAgentConnector_ms: 0.14053797721862793\n",
      "counters:\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-30_17-41-49\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.14053797721862793\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 214.17\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 214.17\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 214.17\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_timesteps_total: 21417\n",
      "  hist_stats:\n",
      "    episode_lengths: [151, 33, 64, 61, 117, 63, 45, 156, 43, 193, 162, 94, 28, 110,\n",
      "      170, 230, 198, 170, 174, 142, 13, 263, 170, 195, 135, 131, 127, 151, 86, 242,\n",
      "      168, 150, 260, 346, 61, 72, 186, 199, 309, 200, 217, 207, 150, 220, 390, 281,\n",
      "      196, 215, 168, 252, 203, 212, 123, 163, 385, 175, 197, 179, 237, 254, 233, 205,\n",
      "      228, 236, 233, 370, 275, 174, 439, 267, 279, 240, 83, 395, 288, 218, 197, 231,\n",
      "      271, 211, 258, 355, 240, 346, 206, 268, 281, 376, 203, 178, 311, 318, 500, 315,\n",
      "      332, 433, 185, 252, 218, 477]\n",
      "    episode_reward: [151.0, 33.0, 64.0, 61.0, 117.0, 63.0, 45.0, 156.0, 43.0, 193.0,\n",
      "      162.0, 94.0, 28.0, 110.0, 170.0, 230.0, 198.0, 170.0, 174.0, 142.0, 13.0, 263.0,\n",
      "      170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0, 242.0, 168.0, 150.0, 260.0,\n",
      "      346.0, 61.0, 72.0, 186.0, 199.0, 309.0, 200.0, 217.0, 207.0, 150.0, 220.0, 390.0,\n",
      "      281.0, 196.0, 215.0, 168.0, 252.0, 203.0, 212.0, 123.0, 163.0, 385.0, 175.0,\n",
      "      197.0, 179.0, 237.0, 254.0, 233.0, 205.0, 228.0, 236.0, 233.0, 370.0, 275.0,\n",
      "      174.0, 439.0, 267.0, 279.0, 240.0, 83.0, 395.0, 288.0, 218.0, 197.0, 231.0,\n",
      "      271.0, 211.0, 258.0, 355.0, 240.0, 346.0, 206.0, 268.0, 281.0, 376.0, 203.0,\n",
      "      178.0, 311.0, 318.0, 500.0, 315.0, 332.0, 433.0, 185.0, 252.0, 218.0, 477.0]\n",
      "  num_episodes: 13\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08564336127229766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.027884488645366776\n",
      "    mean_inference_ms: 0.7604370821748502\n",
      "    mean_raw_obs_processing_ms: 0.2234580923008373\n",
      "episode_len_mean: 214.17\n",
      "episode_media: {}\n",
      "episode_return_max: 500.0\n",
      "episode_return_mean: 214.17\n",
      "episode_return_min: 13.0\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 214.17\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 13\n",
      "episodes_timesteps_total: 21417\n",
      "episodes_total: 377\n",
      "hostname: LAPTOP-5S3FDGMV\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.075\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5697059836438907\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.5627941075592272\n",
      "        kl: 0.006249433456030344\n",
      "        policy_loss: -0.016775176511897194\n",
      "        total_loss: 9.826402447813301\n",
      "        vf_explained_var: 0.03646005616393141\n",
      "        vf_loss: 9.842708912716118\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 6975.5\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_env_steps_sampled: 32000\n",
      "  num_env_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 32000\n",
      "num_agent_steps_sampled_lifetime: 32000\n",
      "num_agent_steps_trained: 32000\n",
      "num_env_steps_sampled: 32000\n",
      "num_env_steps_sampled_lifetime: 32000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 94.77445937747646\n",
      "num_env_steps_trained: 32000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 94.77445937747646\n",
      "num_episodes: 13\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 50.28983050847458\n",
      "  ram_util_percent: 54.606779661016944\n",
      "pid: 2612\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08564336127229766\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.027884488645366776\n",
      "  mean_inference_ms: 0.7604370821748502\n",
      "  mean_raw_obs_processing_ms: 0.2234580923008373\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0\n",
      "    StateBufferConnector_ms: 0.0\n",
      "    ViewRequirementAgentConnector_ms: 0.14053797721862793\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 214.17\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 214.17\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 214.17\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 13\n",
      "  episodes_timesteps_total: 21417\n",
      "  hist_stats:\n",
      "    episode_lengths: [151, 33, 64, 61, 117, 63, 45, 156, 43, 193, 162, 94, 28, 110,\n",
      "      170, 230, 198, 170, 174, 142, 13, 263, 170, 195, 135, 131, 127, 151, 86, 242,\n",
      "      168, 150, 260, 346, 61, 72, 186, 199, 309, 200, 217, 207, 150, 220, 390, 281,\n",
      "      196, 215, 168, 252, 203, 212, 123, 163, 385, 175, 197, 179, 237, 254, 233, 205,\n",
      "      228, 236, 233, 370, 275, 174, 439, 267, 279, 240, 83, 395, 288, 218, 197, 231,\n",
      "      271, 211, 258, 355, 240, 346, 206, 268, 281, 376, 203, 178, 311, 318, 500, 315,\n",
      "      332, 433, 185, 252, 218, 477]\n",
      "    episode_reward: [151.0, 33.0, 64.0, 61.0, 117.0, 63.0, 45.0, 156.0, 43.0, 193.0,\n",
      "      162.0, 94.0, 28.0, 110.0, 170.0, 230.0, 198.0, 170.0, 174.0, 142.0, 13.0, 263.0,\n",
      "      170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0, 242.0, 168.0, 150.0, 260.0,\n",
      "      346.0, 61.0, 72.0, 186.0, 199.0, 309.0, 200.0, 217.0, 207.0, 150.0, 220.0, 390.0,\n",
      "      281.0, 196.0, 215.0, 168.0, 252.0, 203.0, 212.0, 123.0, 163.0, 385.0, 175.0,\n",
      "      197.0, 179.0, 237.0, 254.0, 233.0, 205.0, 228.0, 236.0, 233.0, 370.0, 275.0,\n",
      "      174.0, 439.0, 267.0, 279.0, 240.0, 83.0, 395.0, 288.0, 218.0, 197.0, 231.0,\n",
      "      271.0, 211.0, 258.0, 355.0, 240.0, 346.0, 206.0, 268.0, 281.0, 376.0, 203.0,\n",
      "      178.0, 311.0, 318.0, 500.0, 315.0, 332.0, 433.0, 185.0, 252.0, 218.0, 477.0]\n",
      "  num_episodes: 13\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08564336127229766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.027884488645366776\n",
      "    mean_inference_ms: 0.7604370821748502\n",
      "    mean_raw_obs_processing_ms: 0.2234580923008373\n",
      "time_since_restore: 350.5459644794464\n",
      "time_this_iter_s: 42.209537506103516\n",
      "time_total_s: 350.5459644794464\n",
      "timers:\n",
      "  learn_throughput: 101.461\n",
      "  learn_time_ms: 39424.112\n",
      "  load_throughput: 30925743.779\n",
      "  load_time_ms: 0.129\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4381.868\n",
      "  synch_weights_time_ms: 5.987\n",
      "  training_iteration_time_ms: 43812.71\n",
      "  training_step_time_ms: 43812.346\n",
      "timestamp: 1717126909\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "trial_id: default\n",
      "\n",
      "agent_timesteps_total: 36000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0009965896606445312\n",
      "  StateBufferConnector_ms: 0.0009965896606445312\n",
      "  ViewRequirementAgentConnector_ms: 0.14069485664367676\n",
      "counters:\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-30_17-42-21\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0009965896606445312\n",
      "    StateBufferConnector_ms: 0.0009965896606445312\n",
      "    ViewRequirementAgentConnector_ms: 0.14069485664367676\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 241.76\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 241.76\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 241.76\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_timesteps_total: 24176\n",
      "  hist_stats:\n",
      "    episode_lengths: [94, 28, 110, 170, 230, 198, 170, 174, 142, 13, 263, 170, 195,\n",
      "      135, 131, 127, 151, 86, 242, 168, 150, 260, 346, 61, 72, 186, 199, 309, 200,\n",
      "      217, 207, 150, 220, 390, 281, 196, 215, 168, 252, 203, 212, 123, 163, 385, 175,\n",
      "      197, 179, 237, 254, 233, 205, 228, 236, 233, 370, 275, 174, 439, 267, 279, 240,\n",
      "      83, 395, 288, 218, 197, 231, 271, 211, 258, 355, 240, 346, 206, 268, 281, 376,\n",
      "      203, 178, 311, 318, 500, 315, 332, 433, 185, 252, 218, 477, 250, 379, 343, 500,\n",
      "      232, 422, 500, 319, 283, 341, 278]\n",
      "    episode_reward: [94.0, 28.0, 110.0, 170.0, 230.0, 198.0, 170.0, 174.0, 142.0,\n",
      "      13.0, 263.0, 170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0, 242.0, 168.0, 150.0,\n",
      "      260.0, 346.0, 61.0, 72.0, 186.0, 199.0, 309.0, 200.0, 217.0, 207.0, 150.0, 220.0,\n",
      "      390.0, 281.0, 196.0, 215.0, 168.0, 252.0, 203.0, 212.0, 123.0, 163.0, 385.0,\n",
      "      175.0, 197.0, 179.0, 237.0, 254.0, 233.0, 205.0, 228.0, 236.0, 233.0, 370.0,\n",
      "      275.0, 174.0, 439.0, 267.0, 279.0, 240.0, 83.0, 395.0, 288.0, 218.0, 197.0,\n",
      "      231.0, 271.0, 211.0, 258.0, 355.0, 240.0, 346.0, 206.0, 268.0, 281.0, 376.0,\n",
      "      203.0, 178.0, 311.0, 318.0, 500.0, 315.0, 332.0, 433.0, 185.0, 252.0, 218.0,\n",
      "      477.0, 250.0, 379.0, 343.0, 500.0, 232.0, 422.0, 500.0, 319.0, 283.0, 341.0,\n",
      "      278.0]\n",
      "  num_episodes: 11\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08556793685040344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.027983946573699302\n",
      "    mean_inference_ms: 0.7566084357483988\n",
      "    mean_raw_obs_processing_ms: 0.22114101235236788\n",
      "episode_len_mean: 241.76\n",
      "episode_media: {}\n",
      "episode_return_max: 500.0\n",
      "episode_return_mean: 241.76\n",
      "episode_return_min: 13.0\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 241.76\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 11\n",
      "episodes_timesteps_total: 24176\n",
      "episodes_total: 388\n",
      "hostname: LAPTOP-5S3FDGMV\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.075\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5611617911887425\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.4353354111814531\n",
      "        kl: 0.0077993111053467476\n",
      "        policy_loss: -0.021205216263651207\n",
      "        total_loss: 9.859083155150055\n",
      "        vf_explained_var: -0.10805538219790305\n",
      "        vf_loss: 9.879703426361084\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 7905.5\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_env_steps_sampled: 36000\n",
      "  num_env_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 36000\n",
      "num_agent_steps_sampled_lifetime: 36000\n",
      "num_agent_steps_trained: 36000\n",
      "num_env_steps_sampled: 36000\n",
      "num_env_steps_sampled_lifetime: 36000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 125.20259806113322\n",
      "num_env_steps_trained: 36000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 125.20259806113322\n",
      "num_episodes: 11\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 47.230434782608704\n",
      "  ram_util_percent: 55.2782608695652\n",
      "pid: 2612\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08556793685040344\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.027983946573699302\n",
      "  mean_inference_ms: 0.7566084357483988\n",
      "  mean_raw_obs_processing_ms: 0.22114101235236788\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0009965896606445312\n",
      "    StateBufferConnector_ms: 0.0009965896606445312\n",
      "    ViewRequirementAgentConnector_ms: 0.14069485664367676\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 241.76\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 241.76\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 241.76\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 11\n",
      "  episodes_timesteps_total: 24176\n",
      "  hist_stats:\n",
      "    episode_lengths: [94, 28, 110, 170, 230, 198, 170, 174, 142, 13, 263, 170, 195,\n",
      "      135, 131, 127, 151, 86, 242, 168, 150, 260, 346, 61, 72, 186, 199, 309, 200,\n",
      "      217, 207, 150, 220, 390, 281, 196, 215, 168, 252, 203, 212, 123, 163, 385, 175,\n",
      "      197, 179, 237, 254, 233, 205, 228, 236, 233, 370, 275, 174, 439, 267, 279, 240,\n",
      "      83, 395, 288, 218, 197, 231, 271, 211, 258, 355, 240, 346, 206, 268, 281, 376,\n",
      "      203, 178, 311, 318, 500, 315, 332, 433, 185, 252, 218, 477, 250, 379, 343, 500,\n",
      "      232, 422, 500, 319, 283, 341, 278]\n",
      "    episode_reward: [94.0, 28.0, 110.0, 170.0, 230.0, 198.0, 170.0, 174.0, 142.0,\n",
      "      13.0, 263.0, 170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0, 242.0, 168.0, 150.0,\n",
      "      260.0, 346.0, 61.0, 72.0, 186.0, 199.0, 309.0, 200.0, 217.0, 207.0, 150.0, 220.0,\n",
      "      390.0, 281.0, 196.0, 215.0, 168.0, 252.0, 203.0, 212.0, 123.0, 163.0, 385.0,\n",
      "      175.0, 197.0, 179.0, 237.0, 254.0, 233.0, 205.0, 228.0, 236.0, 233.0, 370.0,\n",
      "      275.0, 174.0, 439.0, 267.0, 279.0, 240.0, 83.0, 395.0, 288.0, 218.0, 197.0,\n",
      "      231.0, 271.0, 211.0, 258.0, 355.0, 240.0, 346.0, 206.0, 268.0, 281.0, 376.0,\n",
      "      203.0, 178.0, 311.0, 318.0, 500.0, 315.0, 332.0, 433.0, 185.0, 252.0, 218.0,\n",
      "      477.0, 250.0, 379.0, 343.0, 500.0, 232.0, 422.0, 500.0, 319.0, 283.0, 341.0,\n",
      "      278.0]\n",
      "  num_episodes: 11\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08556793685040344\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.027983946573699302\n",
      "    mean_inference_ms: 0.7566084357483988\n",
      "    mean_raw_obs_processing_ms: 0.22114101235236788\n",
      "time_since_restore: 382.49912548065186\n",
      "time_this_iter_s: 31.953161001205444\n",
      "time_total_s: 382.49912548065186\n",
      "timers:\n",
      "  learn_throughput: 104.79\n",
      "  learn_time_ms: 38171.409\n",
      "  load_throughput: 34791461.751\n",
      "  load_time_ms: 0.115\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4316.584\n",
      "  synch_weights_time_ms: 5.67\n",
      "  training_iteration_time_ms: 42494.544\n",
      "  training_step_time_ms: 42494.11\n",
      "timestamp: 1717126941\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "trial_id: default\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 40000\n",
      "connector_metrics:\n",
      "  ObsPreprocessorConnector_ms: 0.0009965896606445312\n",
      "  StateBufferConnector_ms: 0.0009965896606445312\n",
      "  ViewRequirementAgentConnector_ms: 0.1237649917602539\n",
      "counters:\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "custom_metrics: {}\n",
      "date: 2024-05-30_17-42-52\n",
      "done: false\n",
      "env_runner_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0009965896606445312\n",
      "    StateBufferConnector_ms: 0.0009965896606445312\n",
      "    ViewRequirementAgentConnector_ms: 0.1237649917602539\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 269.85\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 269.85\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 269.85\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_timesteps_total: 26985\n",
      "  hist_stats:\n",
      "    episode_lengths: [13, 263, 170, 195, 135, 131, 127, 151, 86, 242, 168, 150, 260,\n",
      "      346, 61, 72, 186, 199, 309, 200, 217, 207, 150, 220, 390, 281, 196, 215, 168,\n",
      "      252, 203, 212, 123, 163, 385, 175, 197, 179, 237, 254, 233, 205, 228, 236, 233,\n",
      "      370, 275, 174, 439, 267, 279, 240, 83, 395, 288, 218, 197, 231, 271, 211, 258,\n",
      "      355, 240, 346, 206, 268, 281, 376, 203, 178, 311, 318, 500, 315, 332, 433, 185,\n",
      "      252, 218, 477, 250, 379, 343, 500, 232, 422, 500, 319, 283, 341, 278, 500, 362,\n",
      "      500, 433, 500, 500, 471, 373, 486]\n",
      "    episode_reward: [13.0, 263.0, 170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0,\n",
      "      242.0, 168.0, 150.0, 260.0, 346.0, 61.0, 72.0, 186.0, 199.0, 309.0, 200.0, 217.0,\n",
      "      207.0, 150.0, 220.0, 390.0, 281.0, 196.0, 215.0, 168.0, 252.0, 203.0, 212.0,\n",
      "      123.0, 163.0, 385.0, 175.0, 197.0, 179.0, 237.0, 254.0, 233.0, 205.0, 228.0,\n",
      "      236.0, 233.0, 370.0, 275.0, 174.0, 439.0, 267.0, 279.0, 240.0, 83.0, 395.0,\n",
      "      288.0, 218.0, 197.0, 231.0, 271.0, 211.0, 258.0, 355.0, 240.0, 346.0, 206.0,\n",
      "      268.0, 281.0, 376.0, 203.0, 178.0, 311.0, 318.0, 500.0, 315.0, 332.0, 433.0,\n",
      "      185.0, 252.0, 218.0, 477.0, 250.0, 379.0, 343.0, 500.0, 232.0, 422.0, 500.0,\n",
      "      319.0, 283.0, 341.0, 278.0, 500.0, 362.0, 500.0, 433.0, 500.0, 500.0, 471.0,\n",
      "      373.0, 486.0]\n",
      "  num_episodes: 9\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08555943464494342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02813218625391818\n",
      "    mean_inference_ms: 0.753501536964922\n",
      "    mean_raw_obs_processing_ms: 0.2191208360255464\n",
      "episode_len_mean: 269.85\n",
      "episode_media: {}\n",
      "episode_return_max: 500.0\n",
      "episode_return_mean: 269.85\n",
      "episode_return_min: 13.0\n",
      "episode_reward_max: 500.0\n",
      "episode_reward_mean: 269.85\n",
      "episode_reward_min: 13.0\n",
      "episodes_this_iter: 9\n",
      "episodes_timesteps_total: 26985\n",
      "episodes_total: 397\n",
      "hostname: LAPTOP-5S3FDGMV\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      custom_metrics: {}\n",
      "      diff_num_grad_updates_vs_sampler_policy: 464.5\n",
      "      learner_stats:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_kl_coeff: 0.075\n",
      "        cur_lr: 5.0000000000000016e-05\n",
      "        entropy: 0.5719294448693594\n",
      "        entropy_coeff: 0.0\n",
      "        grad_gnorm: 0.4103775854072263\n",
      "        kl: 0.007559584883330278\n",
      "        policy_loss: -0.021909238077620026\n",
      "        total_loss: 9.878290519919446\n",
      "        vf_explained_var: -0.2862069594603713\n",
      "        vf_loss: 9.899632780013546\n",
      "      model: {}\n",
      "      num_agent_steps_trained: 128.0\n",
      "      num_grad_updates_lifetime: 8835.5\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_env_steps_sampled: 40000\n",
      "  num_env_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 127.0.0.1\n",
      "num_agent_steps_sampled: 40000\n",
      "num_agent_steps_sampled_lifetime: 40000\n",
      "num_agent_steps_trained: 40000\n",
      "num_env_steps_sampled: 40000\n",
      "num_env_steps_sampled_lifetime: 40000\n",
      "num_env_steps_sampled_this_iter: 4000\n",
      "num_env_steps_sampled_throughput_per_sec: 127.92687578287197\n",
      "num_env_steps_trained: 40000\n",
      "num_env_steps_trained_this_iter: 4000\n",
      "num_env_steps_trained_throughput_per_sec: 127.92687578287197\n",
      "num_episodes: 9\n",
      "num_faulty_episodes: 0\n",
      "num_healthy_workers: 1\n",
      "num_in_flight_async_reqs: 0\n",
      "num_remote_worker_restarts: 0\n",
      "num_steps_trained_this_iter: 4000\n",
      "perf:\n",
      "  cpu_util_percent: 46.775\n",
      "  ram_util_percent: 55.07272727272727\n",
      "pid: 2612\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.08555943464494342\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.02813218625391818\n",
      "  mean_inference_ms: 0.753501536964922\n",
      "  mean_raw_obs_processing_ms: 0.2191208360255464\n",
      "sampler_results:\n",
      "  connector_metrics:\n",
      "    ObsPreprocessorConnector_ms: 0.0009965896606445312\n",
      "    StateBufferConnector_ms: 0.0009965896606445312\n",
      "    ViewRequirementAgentConnector_ms: 0.1237649917602539\n",
      "  custom_metrics: {}\n",
      "  episode_len_mean: 269.85\n",
      "  episode_media: {}\n",
      "  episode_return_max: 500.0\n",
      "  episode_return_mean: 269.85\n",
      "  episode_return_min: 13.0\n",
      "  episode_reward_max: 500.0\n",
      "  episode_reward_mean: 269.85\n",
      "  episode_reward_min: 13.0\n",
      "  episodes_this_iter: 9\n",
      "  episodes_timesteps_total: 26985\n",
      "  hist_stats:\n",
      "    episode_lengths: [13, 263, 170, 195, 135, 131, 127, 151, 86, 242, 168, 150, 260,\n",
      "      346, 61, 72, 186, 199, 309, 200, 217, 207, 150, 220, 390, 281, 196, 215, 168,\n",
      "      252, 203, 212, 123, 163, 385, 175, 197, 179, 237, 254, 233, 205, 228, 236, 233,\n",
      "      370, 275, 174, 439, 267, 279, 240, 83, 395, 288, 218, 197, 231, 271, 211, 258,\n",
      "      355, 240, 346, 206, 268, 281, 376, 203, 178, 311, 318, 500, 315, 332, 433, 185,\n",
      "      252, 218, 477, 250, 379, 343, 500, 232, 422, 500, 319, 283, 341, 278, 500, 362,\n",
      "      500, 433, 500, 500, 471, 373, 486]\n",
      "    episode_reward: [13.0, 263.0, 170.0, 195.0, 135.0, 131.0, 127.0, 151.0, 86.0,\n",
      "      242.0, 168.0, 150.0, 260.0, 346.0, 61.0, 72.0, 186.0, 199.0, 309.0, 200.0, 217.0,\n",
      "      207.0, 150.0, 220.0, 390.0, 281.0, 196.0, 215.0, 168.0, 252.0, 203.0, 212.0,\n",
      "      123.0, 163.0, 385.0, 175.0, 197.0, 179.0, 237.0, 254.0, 233.0, 205.0, 228.0,\n",
      "      236.0, 233.0, 370.0, 275.0, 174.0, 439.0, 267.0, 279.0, 240.0, 83.0, 395.0,\n",
      "      288.0, 218.0, 197.0, 231.0, 271.0, 211.0, 258.0, 355.0, 240.0, 346.0, 206.0,\n",
      "      268.0, 281.0, 376.0, 203.0, 178.0, 311.0, 318.0, 500.0, 315.0, 332.0, 433.0,\n",
      "      185.0, 252.0, 218.0, 477.0, 250.0, 379.0, 343.0, 500.0, 232.0, 422.0, 500.0,\n",
      "      319.0, 283.0, 341.0, 278.0, 500.0, 362.0, 500.0, 433.0, 500.0, 500.0, 471.0,\n",
      "      373.0, 486.0]\n",
      "  num_episodes: 9\n",
      "  num_faulty_episodes: 0\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08555943464494342\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.02813218625391818\n",
      "    mean_inference_ms: 0.753501536964922\n",
      "    mean_raw_obs_processing_ms: 0.2191208360255464\n",
      "time_since_restore: 413.7709701061249\n",
      "time_this_iter_s: 31.271844625473022\n",
      "time_total_s: 413.7709701061249\n",
      "timers:\n",
      "  learn_throughput: 107.815\n",
      "  learn_time_ms: 37100.502\n",
      "  load_throughput: 19717024.327\n",
      "  load_time_ms: 0.203\n",
      "  restore_workers_time_ms: 0.0\n",
      "  sample_time_ms: 4264.98\n",
      "  synch_weights_time_ms: 5.501\n",
      "  training_iteration_time_ms: 41371.876\n",
      "  training_step_time_ms: 41371.485\n",
      "timestamp: 1717126972\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "trial_id: default\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "\n",
    "algo = (\n",
    "    PPOConfig()\n",
    "    .env_runners(num_env_runners=1)\n",
    "    .resources(num_gpus=0)\n",
    "    .environment(env=\"CartPole-v1\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()\n",
    "    print(pretty_print(result))\n",
    "\n",
    "    if i % 5 == 0:\n",
    "        checkpoint_dir = algo.save().checkpoint.path\n",
    "        print(f\"Checkpoint saved in directory {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbc1d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
